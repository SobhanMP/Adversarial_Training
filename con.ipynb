{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from src import *\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "ϵ = 8 / 256\n",
    "K = 7\n",
    "retrain = 10\n",
    "epoch_count = 300\n",
    "batch_size = 128\n",
    "pre_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR INPUT\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandomCrop(32, padding=4),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "def trainloadicator(b):\n",
    "    return torch.utils.data.DataLoader(trainset, \n",
    "        batch_size=b,\n",
    "        shuffle=True, num_workers=4, \n",
    "        pin_memory=True, drop_last=True)\n",
    "trainloader = trainloadicator(batch_size)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = StandardScalerLayer(lambda: map(lambda x: x[0], trainloader))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def mkmodel(ϵ=ϵ):\n",
    "    model = WideResNet(34, 10, 10, 0.3)\n",
    "    adv = AdversarialForFree(ϵ, 0, 1)\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('adv', adv),\n",
    "        ('normalizer', norm),\n",
    "        ('resnet', model)])).cuda()\n",
    "\n",
    "imgsize = images.size()[1:]\n",
    "imgsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting batch size to 128\n",
      "train \t 1: 2.0808 22.0% 617.4s\n",
      "val \t 1: 2.1781 21.5% 5.9s\n",
      "adv \t 1: 2.6041 16.4% 120.6s\n",
      "train \t 2: 1.9606 26.5% 606.1s\n",
      "val \t 2: 1.7089 33.5% 5.8s\n",
      "adv \t 2: 2.0658 21.4% 119.5s\n",
      "train \t 3: 1.8441 30.7% 608.6s\n",
      "val \t 3: 1.8288 34.4% 5.8s\n",
      "adv \t 3: 2.4063 19.5% 119.9s\n",
      "train \t 4: 1.8152 31.6% 608.4s\n",
      "val \t 4: 1.5631 42.1% 5.8s\n",
      "adv \t 4: 1.9891 25.1% 119.6s\n",
      "train \t 5: 1.7979 32.4% 606.1s\n",
      "val \t 5: 1.6276 36.5% 5.8s\n",
      "adv \t 5: 2.2220 19.2% 119.3s\n"
     ]
    }
   ],
   "source": [
    "train_loader = None\n",
    "train_batch = None\n",
    "\n",
    "model = mkmodel()\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(math.ceil(epoch_count / K)):  # loop over the dataset multiple times\n",
    "                   \n",
    "    b = 128\n",
    "    if train_batch != b:\n",
    "        print(f'setting batch size to {b}')\n",
    "        train_loader = trainloadicator(b)\n",
    "        train_batch = b\n",
    "\n",
    "    logs = Logisticator()\n",
    "    model.train()\n",
    "                   \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = map(lambda x: x.cuda(), data)\n",
    "        for k in range(K):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            model.adv.step()\n",
    "                     \n",
    "            acc = accuracy(outputs, labels)\n",
    "            logs.add(acc, loss.item(), inputs.size(0))\n",
    "    \n",
    "    print(f'train \\t {epoch + 1}: {logs}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.train(False)\n",
    "    # valdiation loss\n",
    "    with torch.no_grad():\n",
    "        logs = Logisticator()\n",
    "        for data in testloader:\n",
    "            inputs, labels = map(lambda x: x.cuda(), data)\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "            acc = accuracy(outputs, labels)\n",
    "            logs.add(acc, loss.item(), inputs.size(0))\n",
    "            \n",
    "        print(f'val \\t {epoch + 1}: {logs}')\n",
    "    \n",
    "    # adv loss\n",
    "    logs = Logisticator()\n",
    "    for data in testloader:\n",
    "        inputs, labels = map(lambda x: x.cuda(), data)\n",
    "        noise = PGK(model, lambda x: F.cross_entropy(x, labels), inputs, ϵ, K)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs + noise)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "            acc = accuracy(outputs, labels)\n",
    "            logs.add(acc, loss.item(), inputs.size(0))\n",
    "    print(f'adv \\t {epoch + 1}: {logs}')\n",
    "    \n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
