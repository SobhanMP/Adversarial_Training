{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import OrderedDict, defaultdict\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from src import *\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=True)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "!mkdir -p figures\n",
    "!mkdir -p snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# PGK\n",
    "ϵ = 8 / 256\n",
    "ϵ_s = 2 / 256\n",
    "\n",
    "\n",
    "val_K = 10\n",
    "EPOCHS = 200\n",
    "TEST_EVERY = 40\n",
    "\n",
    "batch_size = 128\n",
    "pre_train = False\n",
    "\n",
    "small = False\n",
    "training_with_replay_Ks = [1, 4, 10, 20]\n",
    "free_Ks = [1, 2, 4, 10, 20]\n",
    "\n",
    "    \n",
    "PGD_Ks = [1, 2, 7]\n",
    "\n",
    "\n",
    "\n",
    "attack_names = ['FSM', 'PGD-20', 'PGD-100', 'CW-100']\n",
    "attacks = [\n",
    "     *[PGD(K, ϵ, 2.5 * ϵ/K) for K in [1, 20, 100]],\n",
    "     CW(100, 1e4, ϵ, 2.5 * ϵ/ 100)]\n",
    "    \n",
    "    \n",
    "if small:\n",
    "    EPOCHS = 5\n",
    "    TEST_EVERY = 5\n",
    "    training_with_replay_Ks = [1, 5]\n",
    "    free_Ks = [1, 5]\n",
    "    attack_names = ['FSM', 'PGD-2', 'CW-2']\n",
    "    attacks = [\n",
    "         *[PGD(K, ϵ, 2.5 * ϵ/K) for K in [1, 2]],\n",
    "         CW(2, 1e4, ϵ, 2.5 * ϵ/ 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(EPOCHS == K * int(EPOCHS / K) for K in training_with_replay_Ks)\n",
    "assert all(EPOCHS == K * int(EPOCHS / K) for K in free_Ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR INPUT\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandomCrop(32, padding=4),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=True, transform=transform)\n",
    "if small:\n",
    "    trainset = torch.utils.data.Subset(trainset, range(batch_size))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, num_workers=4, \n",
    "        pin_memory=True, drop_last=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "\n",
    "if small:\n",
    "    testset = torch.utils.data.Subset(testset, range(batch_size))\n",
    "    \n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = StandardScalerLayer(lambda: map(lambda x: x[0], trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model(ϵ=ϵ, K=1):\n",
    "    model = WideResNet(28, 10, 10, 0.1)\n",
    "    adv = AdversarialForFree(ϵ, 0, 1)\n",
    "    if ϵ not in [0, False]:\n",
    "        l = [('adv', adv)]\n",
    "    else:\n",
    "        l = []\n",
    "    l.extend([\n",
    "        ('normalizer', norm),\n",
    "        ('resnet', model)])\n",
    "    \n",
    "    model = nn.Sequential(OrderedDict(l)).cuda()\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                          lr=0.1,\n",
    "                          nesterov=True, \n",
    "                          momentum=0.9)\n",
    "    \n",
    "    scheduler =  optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60//K, 120//K, 160//K], gamma=0.2)\n",
    "    \n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "imgsize = images.size()[1:]\n",
    "imgsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "training with 1 replays------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train \t 1: 1.4239 48.3% 70.5s\n",
      "train \t 2: 0.9001 68.2% 70.4s\n",
      "train \t 3: 0.7045 75.5% 70.5s\n",
      "train \t 4: 0.5947 79.3% 70.5s\n",
      "train \t 5: 0.5242 81.7% 70.5s\n",
      "train \t 6: 0.4688 83.9% 71.0s\n",
      "train \t 7: 0.4235 85.3% 70.5s\n",
      "train \t 8: 0.3927 86.3% 70.5s\n",
      "train \t 9: 0.3571 87.6% 70.5s\n",
      "train \t 10: 0.3299 88.6% 70.5s\n",
      "train \t 11: 0.3091 89.2% 70.5s\n",
      "train \t 12: 0.2870 89.9% 70.5s\n",
      "train \t 13: 0.2657 90.8% 70.6s\n",
      "train \t 14: 0.2475 91.4% 70.9s\n",
      "train \t 15: 0.2348 91.8% 70.5s\n",
      "train \t 16: 0.2170 92.3% 70.5s\n",
      "train \t 17: 0.2089 92.7% 70.9s\n",
      "train \t 18: 0.1956 93.2% 70.5s\n",
      "train \t 19: 0.1847 93.6% 70.5s\n",
      "train \t 20: 0.1736 93.8% 70.5s\n",
      "train \t 21: 0.1627 94.2% 70.5s\n",
      "train \t 22: 0.1493 94.7% 70.5s\n",
      "train \t 23: 0.1438 94.9% 70.5s\n",
      "train \t 24: 0.1347 95.2% 70.5s\n",
      "train \t 25: 0.1252 95.5% 70.5s\n",
      "train \t 26: 0.1222 95.7% 70.5s\n",
      "train \t 27: 0.1158 95.9% 70.5s\n",
      "train \t 28: 0.1060 96.2% 70.5s\n",
      "train \t 29: 0.1054 96.3% 70.5s\n",
      "train \t 30: 0.0952 96.6% 70.5s\n",
      "train \t 31: 0.0900 96.8% 70.5s\n",
      "train \t 32: 0.0903 96.7% 70.5s\n",
      "train \t 33: 0.0831 97.1% 70.5s\n",
      "train \t 34: 0.0788 97.2% 70.9s\n",
      "train \t 35: 0.0738 97.4% 70.5s\n",
      "train \t 36: 0.0701 97.5% 70.9s\n",
      "train \t 37: 0.0694 97.5% 70.5s\n",
      "train \t 38: 0.0697 97.6% 70.5s\n",
      "train \t 39: 0.0634 97.8% 70.5s\n",
      "train \t 40: 0.0600 97.8% 70.9s\n",
      "val \t 40: 0.4198 90.4% 4.9s\n",
      "train \t 41: 0.0579 98.0% 70.5s\n",
      "train \t 42: 0.0519 98.1% 70.5s\n",
      "train \t 43: 0.0508 98.2% 70.9s\n",
      "train \t 44: 0.0469 98.3% 70.4s\n",
      "train \t 45: 0.0510 98.2% 70.5s\n",
      "train \t 46: 0.0484 98.3% 70.5s\n",
      "train \t 47: 0.0438 98.5% 70.9s\n",
      "train \t 48: 0.0399 98.6% 70.5s\n",
      "train \t 49: 0.0418 98.6% 70.5s\n",
      "train \t 50: 0.0426 98.5% 70.5s\n",
      "train \t 51: 0.0351 98.7% 70.5s\n",
      "train \t 52: 0.0379 98.7% 70.5s\n",
      "train \t 53: 0.0376 98.6% 70.5s\n",
      "train \t 54: 0.0374 98.7% 70.5s\n",
      "train \t 55: 0.0336 98.8% 70.6s\n",
      "train \t 56: 0.0348 98.8% 70.5s\n",
      "train \t 57: 0.0322 98.9% 70.5s\n",
      "train \t 58: 0.0273 99.0% 70.5s\n",
      "train \t 59: 0.0270 99.1% 70.5s\n",
      "train \t 60: 0.0292 99.0% 70.5s\n",
      "train \t 61: 0.0148 99.5% 70.5s\n",
      "train \t 62: 0.0080 99.8% 70.5s\n",
      "train \t 63: 0.0067 99.8% 70.5s\n",
      "train \t 64: 0.0067 99.8% 70.9s\n",
      "train \t 65: 0.0057 99.8% 70.9s\n",
      "train \t 66: 0.0050 99.9% 70.8s\n",
      "train \t 67: 0.0049 99.9% 70.9s\n",
      "train \t 68: 0.0046 99.9% 70.9s\n",
      "train \t 69: 0.0044 99.9% 70.4s\n",
      "train \t 70: 0.0046 99.9% 70.9s\n",
      "train \t 71: 0.0041 99.9% 70.5s\n",
      "train \t 72: 0.0039 99.9% 70.5s\n",
      "train \t 73: 0.0043 99.9% 70.5s\n",
      "train \t 74: 0.0029 99.9% 70.5s\n",
      "train \t 75: 0.0034 99.9% 70.5s\n",
      "train \t 76: 0.0033 99.9% 70.5s\n",
      "train \t 77: 0.0027 99.9% 70.5s\n",
      "train \t 78: 0.0030 99.9% 70.5s\n",
      "train \t 79: 0.0031 99.9% 70.5s\n",
      "train \t 80: 0.0032 99.9% 70.5s\n",
      "val \t 80: 0.3804 93.4% 4.9s\n",
      "train \t 81: 0.0027 99.9% 70.4s\n",
      "train \t 82: 0.0027 99.9% 70.4s\n",
      "train \t 83: 0.0025 99.9% 70.5s\n",
      "train \t 84: 0.0025 99.9% 70.5s\n",
      "train \t 85: 0.0030 99.9% 70.5s\n",
      "train \t 86: 0.0028 99.9% 70.5s\n",
      "train \t 87: 0.0023 99.9% 70.5s\n",
      "train \t 88: 0.0023 99.9% 70.5s\n",
      "train \t 89: 0.0024 99.9% 70.5s\n",
      "train \t 90: 0.0021 100.0% 70.5s\n",
      "train \t 91: 0.0031 99.9% 70.5s\n",
      "train \t 92: 0.0026 99.9% 70.5s\n",
      "train \t 93: 0.0022 99.9% 70.5s\n",
      "train \t 94: 0.0024 99.9% 70.5s\n",
      "train \t 95: 0.0022 99.9% 70.5s\n",
      "train \t 96: 0.0017 100.0% 70.5s\n",
      "train \t 97: 0.0023 99.9% 70.9s\n",
      "train \t 98: 0.0015 100.0% 70.5s\n",
      "train \t 99: 0.0019 100.0% 70.5s\n",
      "train \t 100: 0.0014 100.0% 70.9s\n",
      "train \t 101: 0.0018 100.0% 70.5s\n",
      "train \t 102: 0.0018 100.0% 70.9s\n",
      "train \t 103: 0.0017 100.0% 70.4s\n",
      "train \t 104: 0.0018 100.0% 70.5s\n",
      "train \t 105: 0.0017 100.0% 70.5s\n",
      "train \t 106: 0.0021 99.9% 70.5s\n",
      "train \t 107: 0.0018 99.9% 70.9s\n",
      "train \t 108: 0.0016 100.0% 70.5s\n",
      "train \t 109: 0.0021 99.9% 70.5s\n",
      "train \t 110: 0.0019 99.9% 70.9s\n",
      "train \t 111: 0.0018 100.0% 70.9s\n",
      "train \t 112: 0.0022 99.9% 70.9s\n",
      "train \t 113: 0.0016 100.0% 70.5s\n",
      "train \t 114: 0.0017 100.0% 70.5s\n",
      "train \t 115: 0.0013 100.0% 70.5s\n",
      "train \t 116: 0.0018 100.0% 70.6s\n",
      "train \t 117: 0.0016 100.0% 70.5s\n",
      "train \t 118: 0.0013 100.0% 70.9s\n",
      "train \t 119: 0.0015 100.0% 70.9s\n",
      "train \t 120: 0.0016 100.0% 70.9s\n",
      "val \t 120: 0.4152 93.6% 4.9s\n",
      "train \t 121: 0.0017 99.9% 70.5s\n",
      "train \t 122: 0.0013 100.0% 70.9s\n",
      "train \t 123: 0.0013 100.0% 70.9s\n",
      "train \t 124: 0.0013 100.0% 70.5s\n",
      "train \t 125: 0.0014 100.0% 70.5s\n",
      "train \t 126: 0.0016 100.0% 70.8s\n",
      "train \t 127: 0.0012 100.0% 70.5s\n",
      "train \t 128: 0.0012 100.0% 70.5s\n",
      "train \t 129: 0.0012 100.0% 70.5s\n",
      "train \t 130: 0.0017 99.9% 70.5s\n",
      "train \t 131: 0.0014 100.0% 70.5s\n",
      "train \t 132: 0.0015 100.0% 70.5s\n",
      "train \t 133: 0.0011 100.0% 70.9s\n",
      "train \t 134: 0.0013 100.0% 70.5s\n",
      "train \t 135: 0.0011 100.0% 70.5s\n",
      "train \t 136: 0.0008 100.0% 70.5s\n",
      "train \t 137: 0.0015 100.0% 70.5s\n",
      "train \t 138: 0.0011 100.0% 70.9s\n",
      "train \t 139: 0.0009 100.0% 70.5s\n",
      "train \t 140: 0.0011 100.0% 70.5s\n",
      "train \t 141: 0.0010 100.0% 70.9s\n",
      "train \t 142: 0.0013 100.0% 70.9s\n",
      "train \t 143: 0.0011 100.0% 70.9s\n",
      "train \t 144: 0.0011 100.0% 70.6s\n",
      "train \t 145: 0.0010 100.0% 70.5s\n",
      "train \t 146: 0.0011 100.0% 70.9s\n",
      "train \t 147: 0.0015 100.0% 70.5s\n",
      "train \t 148: 0.0011 100.0% 70.9s\n",
      "train \t 149: 0.0013 100.0% 70.5s\n",
      "train \t 150: 0.0011 100.0% 70.5s\n",
      "train \t 151: 0.0014 100.0% 70.9s\n",
      "train \t 152: 0.0008 100.0% 70.5s\n",
      "train \t 153: 0.0012 100.0% 70.5s\n",
      "train \t 154: 0.0010 100.0% 70.5s\n",
      "train \t 155: 0.0010 100.0% 70.5s\n",
      "train \t 156: 0.0008 100.0% 70.9s\n",
      "train \t 157: 0.0012 100.0% 70.6s\n",
      "train \t 158: 0.0012 100.0% 70.5s\n",
      "train \t 159: 0.0011 100.0% 70.5s\n",
      "train \t 160: 0.0011 100.0% 70.5s\n",
      "val \t 160: 0.4077 93.7% 4.9s\n",
      "train \t 161: 0.0012 100.0% 70.5s\n",
      "train \t 162: 0.0012 100.0% 70.5s\n",
      "train \t 163: 0.0011 100.0% 70.5s\n",
      "train \t 164: 0.0009 100.0% 70.5s\n",
      "train \t 165: 0.0013 100.0% 70.5s\n",
      "train \t 166: 0.0010 100.0% 70.5s\n",
      "train \t 167: 0.0010 100.0% 70.9s\n",
      "train \t 168: 0.0012 100.0% 70.5s\n",
      "train \t 169: 0.0007 100.0% 70.5s\n",
      "train \t 170: 0.0010 100.0% 70.5s\n",
      "train \t 171: 0.0011 100.0% 70.5s\n",
      "train \t 172: 0.0011 100.0% 70.5s\n",
      "train \t 173: 0.0011 100.0% 70.5s\n",
      "train \t 174: 0.0008 100.0% 70.5s\n",
      "train \t 175: 0.0009 100.0% 70.5s\n",
      "train \t 176: 0.0011 100.0% 70.5s\n",
      "train \t 177: 0.0009 100.0% 70.5s\n",
      "train \t 178: 0.0010 100.0% 70.5s\n",
      "train \t 179: 0.0010 100.0% 70.5s\n",
      "train \t 180: 0.0011 100.0% 70.5s\n",
      "train \t 181: 0.0010 100.0% 70.5s\n",
      "train \t 182: 0.0010 100.0% 70.5s\n",
      "train \t 183: 0.0012 100.0% 70.5s\n",
      "train \t 184: 0.0007 100.0% 70.5s\n",
      "train \t 185: 0.0012 100.0% 70.5s\n",
      "train \t 186: 0.0008 100.0% 70.5s\n",
      "train \t 187: 0.0011 100.0% 70.9s\n",
      "train \t 188: 0.0009 100.0% 70.5s\n",
      "train \t 189: 0.0011 100.0% 70.9s\n",
      "train \t 190: 0.0007 100.0% 70.5s\n",
      "train \t 191: 0.0007 100.0% 70.5s\n",
      "train \t 192: 0.0011 100.0% 70.5s\n",
      "train \t 193: 0.0008 100.0% 70.5s\n",
      "train \t 194: 0.0009 100.0% 70.5s\n",
      "train \t 195: 0.0008 100.0% 70.5s\n",
      "train \t 196: 0.0013 100.0% 70.9s\n",
      "train \t 197: 0.0009 100.0% 70.5s\n",
      "train \t 198: 0.0008 100.0% 70.5s\n",
      "train \t 199: 0.0013 100.0% 70.5s\n",
      "train \t 200: 0.0008 100.0% 70.5s\n",
      "val \t 200: 0.4120 93.6% 4.9s\n",
      "adv FSM \t\t\t 200: 20.1622 15.3% 18.4s\n",
      "adv PGD-20 \t\t\t 200: 56.2116 0.6% 273.6s\n",
      "adv PGD-100 \t\t\t 200: 57.1002 0.5% 1348.4s\n",
      "adv CW-100 \t\t\t 200: 1.5535 0.4% 1318.4s\n",
      "Finished Training\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "training with 2 replays------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train \t 1: 1.6671 38.1% 140.8s\n",
      "train \t 2: 1.3077 51.7% 140.7s\n",
      "train \t 3: 1.1319 58.6% 140.7s\n",
      "train \t 4: 1.0202 62.6% 140.7s\n",
      "train \t 5: 0.9462 65.2% 140.7s\n",
      "train \t 6: 0.8731 67.6% 140.7s\n",
      "train \t 7: 0.8161 69.6% 140.7s\n",
      "train \t 8: 0.7718 71.1% 140.7s\n",
      "train \t 9: 0.7366 72.3% 140.7s\n",
      "train \t 10: 0.6896 73.9% 140.7s\n",
      "train \t 11: 0.6638 74.9% 140.7s\n",
      "train \t 12: 0.6392 75.9% 140.7s\n",
      "train \t 13: 0.6045 76.9% 140.7s\n",
      "train \t 14: 0.5729 78.1% 140.7s\n",
      "train \t 15: 0.5407 79.2% 140.6s\n",
      "train \t 16: 0.5204 79.9% 141.0s\n",
      "train \t 17: 0.4868 81.1% 140.6s\n",
      "train \t 18: 0.4742 81.6% 140.6s\n",
      "train \t 19: 0.4495 82.5% 140.6s\n",
      "train \t 20: 0.4274 83.3% 140.7s\n",
      "val \t 20: 0.3787 86.8% 4.9s\n",
      "train \t 21: 0.4109 83.8% 140.6s\n",
      "train \t 22: 0.3806 84.9% 140.6s\n",
      "train \t 23: 0.3639 85.5% 140.6s\n",
      "train \t 24: 0.3358 86.4% 140.6s\n",
      "train \t 25: 0.3331 86.7% 140.7s\n",
      "train \t 26: 0.2986 87.9% 140.6s\n",
      "train \t 27: 0.2721 88.9% 140.6s\n",
      "train \t 28: 0.2631 89.3% 140.6s\n",
      "train \t 29: 0.2440 90.1% 140.5s\n",
      "train \t 30: 0.2253 90.7% 141.3s\n",
      "train \t 31: 0.1509 93.9% 140.5s\n",
      "train \t 32: 0.1281 94.7% 140.6s\n",
      "train \t 33: 0.1140 95.2% 140.6s\n",
      "train \t 34: 0.1067 95.6% 141.4s\n",
      "train \t 35: 0.1030 95.7% 140.6s\n",
      "train \t 36: 0.0963 95.9% 140.6s\n",
      "train \t 37: 0.0910 96.3% 140.6s\n",
      "train \t 38: 0.0849 96.5% 140.6s\n",
      "train \t 39: 0.0822 96.6% 140.6s\n",
      "train \t 40: 0.0816 96.6% 140.6s\n",
      "val \t 40: 0.3660 90.1% 4.9s\n",
      "train \t 41: 0.0793 96.7% 140.6s\n",
      "train \t 42: 0.0765 96.9% 140.7s\n",
      "train \t 43: 0.0740 96.9% 140.6s\n",
      "train \t 44: 0.0705 97.1% 140.6s\n",
      "train \t 45: 0.0664 97.3% 141.4s\n",
      "train \t 46: 0.0652 97.3% 140.5s\n",
      "train \t 47: 0.0651 97.4% 140.6s\n",
      "train \t 48: 0.0655 97.3% 140.6s\n",
      "train \t 49: 0.0639 97.4% 140.6s\n",
      "train \t 50: 0.0611 97.5% 140.6s\n",
      "train \t 51: 0.0584 97.7% 140.6s\n",
      "train \t 52: 0.0598 97.5% 140.5s\n",
      "train \t 53: 0.0567 97.7% 140.7s\n",
      "train \t 54: 0.0552 97.8% 140.6s\n",
      "train \t 55: 0.0534 97.8% 141.4s\n",
      "train \t 56: 0.0534 97.8% 140.5s\n",
      "train \t 57: 0.0520 97.9% 140.6s\n",
      "train \t 58: 0.0523 97.9% 140.6s\n",
      "train \t 59: 0.0505 98.0% 141.4s\n",
      "train \t 60: 0.0481 98.1% 140.6s\n",
      "val \t 60: 0.4275 89.8% 4.9s\n",
      "train \t 61: 0.0455 98.2% 140.6s\n",
      "train \t 62: 0.0419 98.3% 140.6s\n",
      "train \t 63: 0.0411 98.4% 140.6s\n",
      "train \t 64: 0.0404 98.4% 140.6s\n",
      "train \t 65: 0.0399 98.4% 141.3s\n",
      "train \t 66: 0.0381 98.5% 141.3s\n",
      "train \t 67: 0.0382 98.5% 140.5s\n",
      "train \t 68: 0.0384 98.5% 141.4s\n",
      "train \t 69: 0.0365 98.5% 141.3s\n",
      "train \t 70: 0.0364 98.6% 140.5s\n",
      "train \t 71: 0.0354 98.6% 141.4s\n",
      "train \t 72: 0.0366 98.5% 140.6s\n",
      "train \t 73: 0.0362 98.6% 140.6s\n",
      "train \t 74: 0.0355 98.6% 141.3s\n",
      "train \t 75: 0.0351 98.6% 141.3s\n",
      "train \t 76: 0.0352 98.6% 141.4s\n",
      "train \t 77: 0.0343 98.6% 140.6s\n",
      "train \t 78: 0.0353 98.6% 140.6s\n",
      "train \t 79: 0.0341 98.7% 140.6s\n",
      "train \t 80: 0.0334 98.7% 140.5s\n",
      "val \t 80: 0.4764 89.8% 4.9s\n",
      "train \t 81: 0.0327 98.8% 140.6s\n",
      "train \t 82: 0.0330 98.7% 140.5s\n",
      "train \t 83: 0.0338 98.7% 140.5s\n",
      "train \t 84: 0.0330 98.7% 140.5s\n",
      "train \t 85: 0.0321 98.8% 141.4s\n",
      "train \t 86: 0.0317 98.7% 140.5s\n",
      "train \t 87: 0.0333 98.7% 140.6s\n",
      "train \t 88: 0.0327 98.8% 140.5s\n",
      "train \t 89: 0.0317 98.8% 140.5s\n",
      "train \t 90: 0.0320 98.7% 140.5s\n",
      "train \t 91: 0.0328 98.7% 140.5s\n",
      "train \t 92: 0.0315 98.8% 140.5s\n",
      "train \t 93: 0.0321 98.8% 140.5s\n",
      "train \t 94: 0.0315 98.8% 140.5s\n",
      "train \t 95: 0.0311 98.8% 140.5s\n",
      "train \t 96: 0.0313 98.8% 140.5s\n",
      "train \t 97: 0.0317 98.8% 140.5s\n",
      "train \t 98: 0.0318 98.8% 141.3s\n",
      "train \t 99: 0.0315 98.8% 140.5s\n",
      "train \t 100: 0.0319 98.8% 140.5s\n",
      "val \t 100: 0.4756 90.0% 4.9s\n",
      "adv FSM \t\t\t 100: 4.7072 43.3% 18.4s\n",
      "adv PGD-20 \t\t\t 100: 7.7010 29.3% 273.2s\n",
      "adv PGD-100 \t\t\t 100: 7.7703 29.0% 1341.6s\n",
      "adv CW-100 \t\t\t 100: 0.9436 29.5% 1329.6s\n",
      "Finished Training\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "training with 4 replays------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train \t 1: 1.8648 30.4% 281.0s\n",
      "train \t 2: 1.6066 39.7% 281.0s\n",
      "train \t 3: 1.4575 45.6% 281.0s\n",
      "train \t 4: 1.3523 49.6% 282.6s\n",
      "train \t 5: 1.2780 52.3% 282.7s\n",
      "train \t 6: 1.2157 54.6% 281.1s\n",
      "train \t 7: 1.1645 56.5% 281.0s\n",
      "train \t 8: 1.1165 58.1% 281.0s\n",
      "train \t 9: 1.0708 59.8% 281.0s\n",
      "train \t 10: 1.0339 61.0% 281.0s\n",
      "val \t 10: 0.6309 80.0% 4.9s\n",
      "train \t 11: 0.9942 62.5% 281.9s\n",
      "train \t 12: 0.9588 63.8% 281.0s\n",
      "train \t 13: 0.9268 64.7% 282.6s\n",
      "train \t 14: 0.8910 65.9% 281.0s\n",
      "train \t 15: 0.8610 67.0% 280.9s\n",
      "train \t 16: 0.7431 71.8% 280.8s\n"
     ]
    }
   ],
   "source": [
    "free_logs = defaultdict(lambda : defaultdict(lambda :[]))\n",
    "\n",
    "for K in free_Ks:\n",
    "    print(f'\\n\\n\\n\\n\\ntraining with {K} replays------------------------\\n\\n\\n\\n')\n",
    "    model, optimizer, scheduler = build_model(K=K)\n",
    "    \n",
    "    for epoch in range(int(EPOCHS / K)):  # loop over the dataset multiple times\n",
    "        logs = train_with_replay(K, model, trainloader, optimizer, epoch,\n",
    "                                after_func=lambda model: model.adv.step())\n",
    "        free_logs[K]['train'].append(logs)\n",
    "        \n",
    "        scheduler.step()\n",
    "        if (epoch * K + K) % TEST_EVERY == 0:\n",
    "\n",
    "            logs = run_val(model, testloader, epoch)\n",
    "            free_logs[K]['test'].append(logs)\n",
    "\n",
    "            # adv loss\n",
    "    run_attacks(free_logs[K], attacks, attack_names, model, testloader, epoch)\n",
    "    \n",
    "\n",
    "    print('Finished Training')\n",
    "    torch.save(model.state_dict(), f\"snapshots/wresnet-cifar-10-free-{K}.pch\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "with open('snapshots/free_logs.pickle', 'wb') as fd:\n",
    "    pickle.dump(holder_to_dict(free_logs), fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard training with replay logs\n",
    "srl = defaultdict(lambda : defaultdict(lambda : []))\n",
    "for K in [training_with_replay_Ks[0]]:\n",
    "    print(f'\\n\\n\\n\\n\\ntraining with {K} replays------------------------\\n\\n\\n\\n')\n",
    "\n",
    "    model, optimizer, scheduler = build_model(False, K=K)\n",
    "        \n",
    "    for epoch in range(int(EPOCHS / K)): # loop over the dataset multiple times\n",
    "            \n",
    "        logs = train_with_replay(K, model, trainloader, optimizer, epoch)\n",
    "        \n",
    "        scheduler.step()\n",
    "        srl[K]['train'].append(logs)\n",
    "        if (epoch * K + K) % TEST_EVERY == 0:\n",
    "            # valdiation loss\n",
    "            logs = run_val(model, testloader, epoch)\n",
    "            srl[K]['test'].append(logs)\n",
    "    run_attacks(srl[K], attacks, \n",
    "                attack_names, model, testloader, epoch)\n",
    "    print('Finished Training')\n",
    "    torch.save(model.state_dict(), f\"wresnet-cifar-10-normal-{K}.pch\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "with open('snapshots/srl.pickle', 'wb') as fd:\n",
    "    pickle.dump(holder_to_dict(srl), fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pgd_logs = defaultdict(lambda : defaultdict(lambda : []))\n",
    "\n",
    "for K in PGD_Ks:\n",
    "    print(f'\\n\\n\\n\\n\\ntraining with {K}-PGD------------------------\\n\\n\\n\\n')\n",
    "    model, optimizer, scheduler = build_model(False)\n",
    "    \n",
    "    attack = PGD(K, ϵ, 2.5 * ϵ / K, early_stopping=False)\n",
    "    \n",
    "    for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        logs = train_with_replay(1, \n",
    "             model, \n",
    "             trainloader, \n",
    "             optimizer,\n",
    "             epoch,\n",
    "             input_func=lambda inputs, labels: attack(model, inputs, labels))\n",
    "        pgd_logs[K]['train'].append(logs)\n",
    "        \n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % TEST_EVERY == 0:\n",
    "    \n",
    "            logs = run_val(model, testloader, epoch)\n",
    "            pgd_logs[K]['test'].append(logs)\n",
    "    run_attacks(pgd_logs[K], attacks, \n",
    "                attack_names, model, testloader, epoch)\n",
    "\n",
    "    print('Finished Training')\n",
    "    torch.save(model.state_dict(), f\"snapshots/wresnet-cifar-10-pgk-{K}.pch\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "with open('snapshots/pgd_logs.pickle', 'wb') as fd:\n",
    "    pickle.dump(holder_to_dict(pgd_logs), fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = lambda x: f'$${x * 100:.2f}\\%$$'\n",
    "d = {}\n",
    "d['Training'] = ['Natural', \n",
    "         *[f'Free $m={K}$' for K in free_Ks],\n",
    "         *[f'{K}-PGD' for K in PGD_Ks]]\n",
    "\n",
    "\n",
    "x = [srl[1]['test'][-1].acc,\n",
    "         *[free_logs[K]['test'][-1].acc for K in free_Ks],\n",
    "         *[pgd_logs[K]['test'][-1].acc for K in PGD_Ks]]\n",
    "\n",
    "d['Natural Images'] = list(map(fmt, x))\n",
    "            \n",
    "for name in attack_names:\n",
    "    n = f'adv_test/{name}'\n",
    "    \n",
    "    x = [srl[1][n][-1].acc]\n",
    "    \n",
    "    for K in free_Ks:\n",
    "        x.append(free_logs[K][n][-1].acc)\n",
    "    \n",
    "    for K in PGD_Ks:\n",
    "        x.append(pgd_logs[K][n][-1].acc)\n",
    "    d[name] = list(map(fmt, x))\n",
    "        \n",
    "tt = lambda x: sum(i.time for i in x)\n",
    "fmt = lambda x: f'$${math.ceil(x / 60)}$$'\n",
    "x = [srl[1]['train'],\n",
    "    *[free_logs[K]['train'] for K in free_Ks],\n",
    "    *[pgd_logs[K]['train'] for K in PGD_Ks]]\n",
    "\n",
    "d['Training Time(M)'] = list(map(lambda x: fmt(tt(x)), x))\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('figures/grid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_latex('figures/grid.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for K in training_with_replay_Ks[1:]:\n",
    "    print(f'\\n\\n\\n\\n\\ntraining with {K} replays------------------------\\n\\n\\n\\n')\n",
    "\n",
    "    model, optimizer, scheduler = build_model(False, K=K)\n",
    "        \n",
    "    for epoch in range(int(EPOCHS / K)): # loop over the dataset multiple times\n",
    "            \n",
    "        logs = train_with_replay(K, model, trainloader, optimizer, epoch)\n",
    "        \n",
    "        scheduler.step()\n",
    "        srl[K]['train'].append(logs)\n",
    "        if (epoch * K + K) % TEST_EVERY == 0:\n",
    "            # valdiation loss\n",
    "            logs = run_val(model, testloader, epoch)\n",
    "            srl[K]['test'].append(logs)\n",
    "    run_attacks(srl[K], attacks, \n",
    "                attack_names, model, testloader, epoch)\n",
    "    print('Finished Training')\n",
    "    torch.save(model.state_dict(), f\"wresnet-cifar-10-normal-{K}.pch\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "with open('snapshots/srl.pickle', 'wb') as fd:\n",
    "    pickle.dump(holder_to_dict(srl), fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax2, ax1) = plt.subplots(ncols=2, figsize=(15,7))\n",
    "\n",
    "y = [srl[K][\"test\"][-1].acc * 100 for K in training_with_replay_Ks]\n",
    "bars = ax1.bar([f'$m={K}$' for K in training_with_replay_Ks], y)\n",
    "for (i, bar) in zip(y, bars):\n",
    "    t = ax1.text(bar.get_x() + bar.get_width() /2 - 0.07 , bar.get_height() + 0.10, f'{i:0.1f}%')\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xlabel('number of replay steps $m$')\n",
    "ax1.set_ylabel('validation accuracy ($\\%$)')\n",
    "\n",
    "ax2.set_ylabel('validation loss (KL)')\n",
    "y = [srl[K][\"test\"][-1].loss for K in training_with_replay_Ks]\n",
    "bars = ax2.bar([f'$m={K}$' for K in training_with_replay_Ks], y)\n",
    "for (i, bar) in zip(y, bars):\n",
    "    t = ax2.text(bar.get_x() + bar.get_width() /2 - 0.07 , bar.get_height() + 0.10, f'{i:0.1f}')\n",
    "def savefig(fig, name, f=['svg', 'pdf', 'png']):\n",
    "    for e in f:\n",
    "        fig.savefig('figures/' + name + '.' + e)\n",
    "savefig(fig, 'cost_of_replay')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
